{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.7\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 50) # Display up to 50 columns at a time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from matplotlib import cm\n",
    "plt.style.use('seaborn')\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 12,5\n",
    "import glob # To read all csv files in the directory\n",
    "import seaborn as sns\n",
    "import calendar\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_recall_fscore_support\n",
    "import itertools\n",
    "import time\n",
    "import xgboost as xgb\n",
    "import glob\n",
    "import pickle as pk\n",
    "from keras.models import load_model\n",
    "import os\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stacked_dataset(models, inputX, scaler = None, fit_scaler = False):\n",
    "    \"\"\"\n",
    "    Input : list of learners, np.array, sklearn object, bool\n",
    "    Output: np.array\n",
    "    The function takes a list of pretrained models, the training observations and eventually a standard scaler to scale \n",
    "    the stacked data. The last boolean is an indicator to tell if the standard scaler ought to be trained or if it has already\n",
    "    been. Then it returns the concatenated predictions\n",
    "    of each and every model in a flattened array. The output will be the input of the level 1 model to train with\n",
    "    trainStack\n",
    "    \"\"\"\n",
    "    stackX = None\n",
    "    for model in models:\n",
    "        # make prediction\n",
    "        yhat = model.predict(inputX)\n",
    "        # stack predictions into [rows, members, probabilities]\n",
    "        if stackX is None:\n",
    "            stackX = yhat\n",
    "        else:\n",
    "            stackX = np.dstack((stackX, yhat))\n",
    "    # flatten predictions to [rows, members x probabilities]\n",
    "    if len(models) > 1:\n",
    "        stackX = stackX.reshape((stackX.shape[1], stackX.shape[2]))\n",
    "        \n",
    "#     print(\"Il y a {0} modèles, le format des observations est : {1} et celui des observations empilées est : {2}\".format(len(models), inputX.shape, stackX.shape))\n",
    "#     print(\"Les cinq premières lignes ressemblent à ceci : {}\".format(stackX[:5,:]))\n",
    "\n",
    "    if scaler is not None:\n",
    "        if fit_scaler:\n",
    "            stackX = scaler.fit_transform(stackX)\n",
    "        else:\n",
    "            stackX = scaler.transform(stackX)\n",
    "    \n",
    "#     print('Les résultats agglomérés des modèles ressemblent à ça : {}'.format(stackX[:5,:]))\n",
    "    return stackX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainStack(first_models, final_model, X_train, y_train, is_MLP = False, epochs = 300, scaler = None, fit_scaler = False):\n",
    "    \"\"\"\n",
    "    Input : list of learners, learner, np.array, np.array, bool, int, sklearn object, bool\n",
    "    Output : learner\n",
    "    The function takes the level 0 trained learners, the level 1 learner to train, the training observations, the \n",
    "    training labels, the boolean telling whether or not the top-level classifier is a Multi-Layer Perceptron and the integer\n",
    "    corresponding to the number of training epochs if we have an MLP. The two last arguments are a respectively a \n",
    "    standard scaler in case we need to scale our data and a boolean telling whether or not it has to be trained.\n",
    "    It returns the level 1 trained model.\n",
    "    \"\"\"\n",
    "    X_stacked = stacked_dataset(first_models, X_train, scaler = scaler, fit_scaler = fit_scaler)\n",
    "    \n",
    "    if is_MLP:\n",
    "        y_train_categ = to_categorical(y_train)\n",
    "#         print(\"Les labels pour l'entrainement ont cette forme : {}\".format(y_train_categ[:5]))\n",
    "        final_model.fit(X_stacked, y_train_categ, epochs = epochs, verbose = 0)\n",
    "    else:\n",
    "        final_model.fit(X_stacked, y_train)\n",
    "    \n",
    "    return final_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictStack(first_models, final_model, X_test, scaler = None):\n",
    "    \"\"\"\n",
    "    Input : list of learners, learner, array-like, sklearn object\n",
    "    Output : array-like\n",
    "    The function takes the first-level trained models, the top-level trained model, the test set and eventually a scaler \n",
    "    that scales the X_test data with a pretrained scaler (trained on the training data) and returns \n",
    "    the predictions of the stack on the test set.\n",
    "    \"\"\"\n",
    "    X_stacked = stacked_dataset(first_models, X_test, scaler = scaler)\n",
    "        \n",
    "    y_predicted = final_model.predict(X_stacked)\n",
    "    return y_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probabilitiesToClass(array):\n",
    "    \"\"\"\n",
    "    Input: np.array of size (n,2)\n",
    "    Output : np.array of size (n,)\n",
    "    The function takes the array whose lines are the observations and column values correspond to the probability of belonging \n",
    "    to a given class. If the probability in a given column is superior to 0.5, then we will return the number of that class. \n",
    "    For instance : [0.33,0.66] for an observation will render [1] and [0.7,0.3] will render [0].\n",
    "    \"\"\"\n",
    "    res = np.array(array.shape[0]*[0])\n",
    "    for i in range(array.shape[0]):\n",
    "        if array[i,1] > array[i,0]:\n",
    "            res[i] = 1\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def displayPerformances(y_test, y_test_pred, y_train, y_train_pred):\n",
    "    \"\"\"\n",
    "    Input: np.array (n,1), np.array (n,1), np.array (N,1), np.array (N,1)\n",
    "    Output : pd.DataFrame\n",
    "    The function takes the labels from the training set, the predicted labels, and the same for the test set. It returns\n",
    "    a DataFrame containing the values on each data set of the precision, the recall and the f1-score.\n",
    "    \"\"\"\n",
    "    test_precision, test_recall, test_f1score, test_support = precision_recall_fscore_support(y_test, y_test_pred, average='weighted')\n",
    "    train_precision, train_recall, train_f1score, train_support = precision_recall_fscore_support(y_train, y_train_pred, average='weighted')\n",
    "    results = {'Precision':[test_precision, train_precision], 'Recall':[test_recall, train_recall], 'F1_score': [test_f1score, train_f1score]}\n",
    "    results = pd.DataFrame(results, index=['Test','Train'])\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Majority vote\n",
    "\n",
    "We did still did not implemant a majority vote on the result given by our level 0 classifiers. Maybe it's time. However, one should keep in mind that ensemble learning and stacking may behave better with **a lot more level 0 classifiers** than currently. Indeed, to take a majority vote, it may seem better to have a lot of different outputs from level 0 learners. Anyway, we'll keep it short with our 3 classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def majorityVote(first_models, X_train):\n",
    "    \"\"\"\n",
    "    Input : list of learners, np.array (N,d)\n",
    "    Output : np.array (N,)\n",
    "    The function takes the level 0 trained learners, and the training observations. It returns the array containing the majority\n",
    "    vote coming from the level-0 classifiers.\n",
    "    \"\"\"\n",
    "    \n",
    "    X_stacked = stacked_dataset(first_models, X_train)\n",
    "    res = np.array(X_stacked.shape[0]*[0])\n",
    "    \n",
    "    for i in range(X_stacked.shape[0]):\n",
    "        line = X_stacked[i,:]\n",
    "        if (line == 1).sum() > (line == 0).sum():\n",
    "            res[i] = 1\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking : logistic regression\n",
    "The level 1 classifier is a logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's import a bunch of the level-0 classifiers\n",
    "level0_models = []\n",
    "level0_nn = []\n",
    "\n",
    "dir_name = 'zero_level_models'\n",
    "file_list = os.listdir(dir_name)\n",
    "\n",
    "for f in file_list:        \n",
    "    if 'model1_' in f:\n",
    "        with open(dir_name+'/'+f,'rb') as file:\n",
    "            pickler = pk.Unpickler(file)\n",
    "            try:\n",
    "                temp = pickler.load()\n",
    "                level0_models.append(temp)\n",
    "            except:\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the datasets. Since the models were trained on the first part of the dataset, we must choose different datasets this\n",
    "# time, for instance the second subsets\n",
    "X_train = pd.read_csv('processed_datasets/observations2.csv', index_col=0)\n",
    "y_train = pd.read_csv('processed_datasets/labels2.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.read_csv('processed_datasets/observations3.csv', index_col=0)\n",
    "y_test = pd.read_csv('processed_datasets/labels3.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_model1 = AdaBoostClassifier();\n",
    "#sc = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    0.2s finished\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "stack1 = trainStack(level0_models, top_model1, X_train.iloc[:5000,:], y_train.iloc[:5000,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    0.2s finished\n"
     ]
    }
   ],
   "source": [
    "y_predicted = predictStack(level0_models, stack1, X_test.iloc[:5000,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    0.3s finished\n"
     ]
    }
   ],
   "source": [
    "# Displaying the results\n",
    "y_train_pred = predictStack(level0_models, stack1, X_train.iloc[:5000,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Test</th>\n",
       "      <td>0.762531</td>\n",
       "      <td>0.7600</td>\n",
       "      <td>0.752974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train</th>\n",
       "      <td>0.758229</td>\n",
       "      <td>0.7596</td>\n",
       "      <td>0.754518</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Precision  Recall  F1_score\n",
       "Test    0.762531  0.7600  0.752974\n",
       "Train   0.758229  0.7596  0.754518"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = displayPerformances(y_test.iloc[:5000,:], y_predicted, y_train.iloc[:5000,:],y_train_pred)\n",
    "res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
