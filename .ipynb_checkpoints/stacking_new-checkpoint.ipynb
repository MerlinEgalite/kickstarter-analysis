{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required libraries\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 50) # Display up to 50 columns at a time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from matplotlib import cm\n",
    "plt.style.use('seaborn')\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 12,5\n",
    "import glob # To read all csv files in the directory\n",
    "import seaborn as sns\n",
    "import calendar\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_recall_fscore_support\n",
    "import itertools\n",
    "import time\n",
    "import xgboost as xgb\n",
    "import glob\n",
    "import pickle as pk\n",
    "from keras.models import load_model\n",
    "import os\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "from sklearn.externals import joblib\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stacked_dataset(models, inputX, scaler = None, fit_scaler = False):\n",
    "    \"\"\"\n",
    "    Input : list of learners, np.array, sklearn object, bool\n",
    "    Output: np.array\n",
    "    The function takes a list of pretrained models, the training observations and eventually a standard scaler to scale \n",
    "    the stacked data. The last boolean is an indicator to tell if the standard scaler ought to be trained or if it has already\n",
    "    been. Then it returns the concatenated predictions\n",
    "    of each and every model in a flattened array. The output will be the input of the level 1 model to train with\n",
    "    trainStack\n",
    "    \"\"\"\n",
    "    stackX = None\n",
    "    for model in models:\n",
    "        # make prediction\n",
    "        yhat = model.predict(inputX)\n",
    "        # stack predictions into [rows, members, probabilities]\n",
    "        if stackX is None:\n",
    "            stackX = yhat\n",
    "        else:\n",
    "            stackX = np.dstack((stackX, yhat))\n",
    "    # flatten predictions to [rows, members x probabilities]\n",
    "    if len(models) > 1:\n",
    "        stackX = stackX.reshape((stackX.shape[1], stackX.shape[2]))\n",
    "        \n",
    "#     print(\"Il y a {0} modèles, le format des observations est : {1} et celui des observations empilées est : {2}\".format(len(models), inputX.shape, stackX.shape))\n",
    "#     print(\"Les cinq premières lignes ressemblent à ceci : {}\".format(stackX[:5,:]))\n",
    "\n",
    "    if scaler is not None:\n",
    "        if fit_scaler:\n",
    "            stackX = scaler.fit_transform(stackX)\n",
    "        else:\n",
    "            stackX = scaler.transform(stackX)\n",
    "    \n",
    "#     print('Les résultats agglomérés des modèles ressemblent à ça : {}'.format(stackX[:5,:]))\n",
    "    return stackX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainStack(first_models, final_model, X_train, y_train, is_MLP = False, epochs = 300, scaler = None, fit_scaler = False):\n",
    "    \"\"\"\n",
    "    Input : list of learners, learner, np.array, np.array, bool, int, sklearn object, bool\n",
    "    Output : learner\n",
    "    The function takes the level 0 trained learners, the level 1 learner to train, the training observations, the \n",
    "    training labels, the boolean telling whether or not the top-level classifier is a Multi-Layer Perceptron and the integer\n",
    "    corresponding to the number of training epochs if we have an MLP. The two last arguments are a respectively a \n",
    "    standard scaler in case we need to scale our data and a boolean telling whether or not it has to be trained.\n",
    "    It returns the level 1 trained model.\n",
    "    \"\"\"\n",
    "    X_stacked = stacked_dataset(first_models, X_train, scaler = scaler, fit_scaler = fit_scaler)\n",
    "    \n",
    "    if is_MLP:\n",
    "        y_train_categ = to_categorical(y_train)\n",
    "#         print(\"Les labels pour l'entrainement ont cette forme : {}\".format(y_train_categ[:5]))\n",
    "        final_model.fit(X_stacked, y_train_categ, epochs = epochs, verbose = 0)\n",
    "    else:\n",
    "        final_model.fit(X_stacked, y_train)\n",
    "    \n",
    "    return final_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictStack(first_models, final_model, X_test, scaler = None):\n",
    "    \"\"\"\n",
    "    Input : list of learners, learner, array-like, sklearn object\n",
    "    Output : array-like\n",
    "    The function takes the first-level trained models, the top-level trained model, the test set and eventually a scaler \n",
    "    that scales the X_test data with a pretrained scaler (trained on the training data) and returns \n",
    "    the predictions of the stack on the test set.\n",
    "    \"\"\"\n",
    "    X_stacked = stacked_dataset(first_models, X_test, scaler = scaler)\n",
    "        \n",
    "    y_predicted = final_model.predict(X_stacked)\n",
    "    return y_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probabilitiesToClass(array):\n",
    "    \"\"\"\n",
    "    Input: np.array of size (n,2)\n",
    "    Output : np.array of size (n,)\n",
    "    The function takes the array whose lines are the observations and column values correspond to the probability of belonging \n",
    "    to a given class. If the probability in a given column is superior to 0.5, then we will return the number of that class. \n",
    "    For instance : [0.33,0.66] for an observation will render [1] and [0.7,0.3] will render [0].\n",
    "    \"\"\"\n",
    "    res = np.array(array.shape[0]*[0])\n",
    "    for i in range(array.shape[0]):\n",
    "        if array[i,1] > array[i,0]:\n",
    "            res[i] = 1\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def displayPerformances(y_test, y_test_pred, y_train, y_train_pred):\n",
    "    \"\"\"\n",
    "    Input: np.array (n,1), np.array (n,1), np.array (N,1), np.array (N,1)\n",
    "    Output : pd.DataFrame\n",
    "    The function takes the labels from the training set, the predicted labels, and the same for the test set. It returns\n",
    "    a DataFrame containing the values on each data set of the precision, the recall and the f1-score.\n",
    "    \"\"\"\n",
    "    test_precision, test_recall, test_f1score, test_support = precision_recall_fscore_support(y_test, y_test_pred, average='weighted')\n",
    "    train_precision, train_recall, train_f1score, train_support = precision_recall_fscore_support(y_train, y_train_pred, average='weighted')\n",
    "    results = {'Precision':[test_precision, train_precision], 'Recall':[test_recall, train_recall], 'F1_score': [test_f1score, train_f1score]}\n",
    "    results = pd.DataFrame(results, index=['Test','Train'])\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Majority vote\n",
    "\n",
    "We did still did not implemant a majority vote on the result given by our level 0 classifiers. Maybe it's time. However, one should keep in mind that ensemble learning and stacking may behave better with **a lot more level 0 classifiers** than currently. Indeed, to take a majority vote, it may seem better to have a lot of different outputs from level 0 learners. Anyway, we'll keep it short with our 3 classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def majorityVote(first_models, X_train):\n",
    "    \"\"\"\n",
    "    Input : list of learners, np.array (N,d)\n",
    "    Output : np.array (N,)\n",
    "    The function takes the level 0 trained learners, and the training observations. It returns the array containing the majority\n",
    "    vote coming from the level-0 classifiers.\n",
    "    \"\"\"\n",
    "    \n",
    "    X_stacked = stacked_dataset(first_models, X_train)\n",
    "    res = np.array(X_stacked.shape[0]*[0])\n",
    "    \n",
    "    for i in range(X_stacked.shape[0]):\n",
    "        line = X_stacked[i,:]\n",
    "        if (line == 1).sum() > (line == 0).sum():\n",
    "            res[i] = 1\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking : logistic regression\n",
    "The level 1 classifier is a logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import a bunch of the level-0 classifiers\n",
    "level0_models = []\n",
    "level0_nn = []\n",
    "\n",
    "dir_name = 'zero_level_models'\n",
    "file_list = os.listdir(dir_name)\n",
    "\n",
    "for f in file_list: \n",
    "    if 'neural_net_model1_' in f:\n",
    "        with open(dir_name+'/'+f,'rb') as file:\n",
    "            try:\n",
    "                temp = load_model(file)\n",
    "                level0_nn.append(temp)\n",
    "            except:\n",
    "                continue\n",
    "    elif 'model1_' in f:\n",
    "        with open(dir_name+'/'+f,'rb') as file:\n",
    "            pickler = pk.Unpickler(file)\n",
    "            try:\n",
    "                temp = pickler.load()\n",
    "                level0_models.append(temp)\n",
    "            except:\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the datasets. Since the models were trained on the first part of the dataset, we must choose different datasets this\n",
    "# time, for instance the second subsets\n",
    "X_train = pd.read_csv('processed_datasets/observations2.csv', index_col=0)\n",
    "y_train = pd.read_csv('processed_datasets/labels2.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.read_csv('processed_datasets/observations3.csv', index_col=0)\n",
    "y_test = pd.read_csv('processed_datasets/labels3.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_model1 = AdaBoostClassifier();\n",
    "#sc = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    0.2s finished\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "stack1 = trainStack(level0_models, top_model1, X_train.iloc[:5000,:], y_train.iloc[:5000,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    0.2s finished\n"
     ]
    }
   ],
   "source": [
    "y_predicted = predictStack(level0_models, stack1, X_test.iloc[:5000,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    0.3s finished\n"
     ]
    }
   ],
   "source": [
    "# Displaying the results\n",
    "y_train_pred = predictStack(level0_models, stack1, X_train.iloc[:5000,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Test</th>\n",
       "      <td>0.762531</td>\n",
       "      <td>0.7600</td>\n",
       "      <td>0.752974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train</th>\n",
       "      <td>0.758229</td>\n",
       "      <td>0.7596</td>\n",
       "      <td>0.754518</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Precision  Recall  F1_score\n",
       "Test    0.762531  0.7600  0.752974\n",
       "Train   0.758229  0.7596  0.754518"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = displayPerformances(y_test.iloc[:5000,:], y_predicted, y_train.iloc[:5000,:],y_train_pred)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking : neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's retrieve the dimension of the input layer\n",
    "n = len(X_train.columns)\n",
    "# let's retrieve the dimension of the output layer\n",
    "m = 1\n",
    "# Neural network\n",
    "top_model2 = Sequential()\n",
    "top_model2.add(Dropout(0.2, input_shape = (n,)))\n",
    "top_model2.add(Dense(1000, activation='relu'))\n",
    "top_model2.add(Dense(1000, activation='relu'))\n",
    "top_model2.add(Dense(1000, activation='relu'))\n",
    "top_model2.add(Dense(500, activation='relu'))\n",
    "top_model2.add(Dense(2, activation='softmax'))\n",
    "top_model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected dropout_1_input to have shape (106,) but got array with shape (16,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-b4e17887133a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstack2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainStack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlevel0_models\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_model2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_MLP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-20-0c70f8089d12>\u001b[0m in \u001b[0;36mtrainStack\u001b[0;34m(first_models, final_model, X_train, y_train, is_MLP, epochs, scaler, fit_scaler)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0my_train_categ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m#         print(\"Les labels pour l'entrainement ont cette forme : {}\".format(y_train_categ[:5]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mfinal_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_stacked\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_categ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mfinal_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_stacked\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    136\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected dropout_1_input to have shape (106,) but got array with shape (16,)"
     ]
    }
   ],
   "source": [
    "stack2 = trainStack(level0_models, top_model2, X_train.iloc[:4000,:], y_train.iloc[:4000,:], is_MLP = True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted2 = predictStack(level0_models, stack2, X_test.iloc[:4000,:])\n",
    "y_pred_flat2 = probabilitiesToClass(y_predicted2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred2 = predictStack(level0_models, stack2, X_train.iloc[:4000,:])\n",
    "y_train_pred_flat2 = probabilitiesToClass(y_train_pred2);        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = displayPerformances(y_test.iloc[:4000,:], y_pred_flat2, y_train.iloc[:4000,:],y_train_pred_flat2)\n",
    "res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
