{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking\n",
    "\n",
    "The idea in this notebook is to implement stacking methods on the kickstarter data. Hopefully, to reach the ultimate score.\n",
    "\n",
    "## Summary\n",
    "\n",
    "<font color = grey>\n",
    "\n",
    "- [With stolen models](#With-stolen-models)\n",
    " 1. [Importing and preprocessing data](#Importing-and-preprocessing-data)\n",
    " 2. [Training simple models withouth dimensionality reduction](#Training-simple-models-without-dimensionality-reduction)\n",
    "    a. [Vanilla logistic regression](#Vanilla-logistic-regression)\n",
    "    b. [Random forest](#Random-forest)\n",
    "    c. [XGBoost](#XGBoost)\n",
    " 3. [Stacking without dimensionality reduction](#Stacking-without-dimensionality-reduction)\n",
    "    a. [Logistic regression](#Logistic-regression)\n",
    " \n",
    "\n",
    "</font>\n",
    "\n",
    "## With stolen models\n",
    "\n",
    "First of all, we will design our functions with the help of the models stolen from the notebook found online. This will help us gain some time. In fact, the author already ran grid searches over her models (extremely expensive), so instead of starting over, we will just take the same models with the same parameters, without dimensionality reduction. After we built our proper functions, we will just have to start over with our one models that perform as accurately as possible on their own in order to get the best results with the full stack.\n",
    "\n",
    "### Importing and preprocessing data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required libraries\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 50) # Display up to 50 columns at a time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from matplotlib import cm\n",
    "plt.style.use('seaborn')\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 12,5\n",
    "import glob # To read all csv files in the directory\n",
    "import seaborn as sns\n",
    "import calendar\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_recall_fscore_support\n",
    "import itertools\n",
    "import time\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([pd.read_csv(f) for f in glob.glob('D:/Utilisateurs/Bastien/Documents/Cours/CentraleSupelec/Electifs/Machine Learning/Projet/kickstarter-analysis/data/Kickstarter*.csv')], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping columns that are mostly null\n",
    "df.drop(['friends', 'is_backing', 'is_starred', 'permissions'], axis=1, inplace=True)\n",
    "\n",
    "# Dropping columns that aren't useful\n",
    "df.drop(['converted_pledged_amount', 'creator', 'currency', 'currency_symbol', 'currency_trailing_code', 'current_currency', 'fx_rate', 'photo', 'pledged', 'profile', 'slug', 'source_url', 'spotlight', 'state_changed_at', 'urls', 'usd_type'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting dates from unix to datetime\n",
    "cols_to_convert = ['created_at', 'deadline', 'launched_at']\n",
    "for c in cols_to_convert:\n",
    "    df[c] = pd.to_datetime(df[c], origin='unix', unit='s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count length of each blurb\n",
    "df['blurb_length'] = df['blurb'].str.split().str.len()\n",
    "\n",
    "# Drop blurb variable\n",
    "df.drop('blurb', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the relevant sub-category section from the string\n",
    "f = lambda x: x['category'].split('/')[1].split('\",\"position')[0]\n",
    "df['sub_category'] = df.apply(f, axis=1)\n",
    "\n",
    "# Extracting the relevant category section from the string, and replacing the original category variable\n",
    "f = lambda x: x['category'].split('\"slug\":\"')[1].split('/')[0]\n",
    "df['category'] = df.apply(f, axis=1)\n",
    "f = lambda x: x['category'].split('\",\"position\"')[0] # Some categories do not have a sub-category, so do not have a '/' to split with\n",
    "df['category'] = df.apply(f, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('disable_communication', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate new column 'usd_goal' as goal * static_usd_rate\n",
    "df['usd_goal'] = round(df['goal'] * df['static_usd_rate'],2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping goal and static_usd_rate\n",
    "df.drop(['goal', 'static_usd_rate'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping location\n",
    "df.drop('location', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count length of each name\n",
    "df['name_length'] = df['name'].str.split().str.len()\n",
    "# Drop name variable\n",
    "df.drop('name', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['usd_pledged'] = round(df['usd_pledged'],2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time between creating and launching a project\n",
    "df['creation_to_launch_days'] = df['launched_at'] - df['created_at']\n",
    "df['creation_to_launch_days'] = df['creation_to_launch_days'].dt.round('d').dt.days # Rounding to nearest days, then showing as number only\n",
    "# Or could show as number of hours:\n",
    "# df['creation_to_launch_hours'] = df['launched_at'] - df['created_at']\n",
    "# df['creation_to_launch_hours'] = df['creation_to_launch_hours'].dt.round('h') / np.timedelta64(1, 'h') \n",
    "\n",
    "# Campaign length\n",
    "df['campaign_days'] = df['deadline'] - df['launched_at']\n",
    "df['campaign_days'] = df['campaign_days'].dt.round('d').dt.days # Rounding to nearest days, then showing as number only\n",
    "\n",
    "# Launch day of week\n",
    "df['launch_day'] = df['launched_at'].dt.weekday_name\n",
    "\n",
    "# Deadline day of week\n",
    "df['deadline_day'] = df['deadline'].dt.weekday_name\n",
    "\n",
    "# Launch month\n",
    "df['launch_month'] = df['launched_at'].dt.month_name()\n",
    "\n",
    "# Deadline month\n",
    "df['deadline_month'] = df['deadline'].dt.month_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch time\n",
    "df['launch_hour'] = df['launched_at'].dt.hour # Extracting hour from launched_at\n",
    "\n",
    "def two_hour_launch(row):\n",
    "    '''Creates two hour bins from the launch_hour column'''\n",
    "    if row['launch_hour'] in (0,1):\n",
    "        return '12am-2am'\n",
    "    if row['launch_hour'] in (2,3):\n",
    "        return '2am-4am'\n",
    "    if row['launch_hour'] in (4,5):\n",
    "        return '4am-6am'\n",
    "    if row['launch_hour'] in (6,7):\n",
    "        return '6am-8am'\n",
    "    if row['launch_hour'] in (8,9):\n",
    "        return '8am-10am'\n",
    "    if row['launch_hour'] in (10,11):\n",
    "        return '10am-12pm'\n",
    "    if row['launch_hour'] in (12,13):\n",
    "        return '12pm-2pm'\n",
    "    if row['launch_hour'] in (14,15):\n",
    "        return '2pm-4pm'\n",
    "    if row['launch_hour'] in (16,17):\n",
    "        return '4pm-6pm'\n",
    "    if row['launch_hour'] in (18,19):\n",
    "        return '6pm-8pm'\n",
    "    if row['launch_hour'] in (20,21):\n",
    "        return '8pm-10pm'\n",
    "    if row['launch_hour'] in (22,23):\n",
    "        return '10pm-12am'\n",
    "    \n",
    "df['launch_time'] = df.apply(two_hour_launch, axis=1) # Calculates bins from launch_time\n",
    "\n",
    "df.drop('launch_hour', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deadline time\n",
    "df['deadline_hour'] = df['deadline'].dt.hour # Extracting hour from deadline\n",
    "\n",
    "def two_hour_deadline(row):\n",
    "    '''Creates two hour bins from the deadline_hour column'''\n",
    "    if row['deadline_hour'] in (0,1):\n",
    "        return '12am-2am'\n",
    "    if row['deadline_hour'] in (2,3):\n",
    "        return '2am-4am'\n",
    "    if row['deadline_hour'] in (4,5):\n",
    "        return '4am-6am'\n",
    "    if row['deadline_hour'] in (6,7):\n",
    "        return '6am-8am'\n",
    "    if row['deadline_hour'] in (8,9):\n",
    "        return '8am-10am'\n",
    "    if row['deadline_hour'] in (10,11):\n",
    "        return '10am-12pm'\n",
    "    if row['deadline_hour'] in (12,13):\n",
    "        return '12pm-2pm'\n",
    "    if row['deadline_hour'] in (14,15):\n",
    "        return '2pm-4pm'\n",
    "    if row['deadline_hour'] in (16,17):\n",
    "        return '4pm-6pm'\n",
    "    if row['deadline_hour'] in (18,19):\n",
    "        return '6pm-8pm'\n",
    "    if row['deadline_hour'] in (20,21):\n",
    "        return '8pm-10pm'\n",
    "    if row['deadline_hour'] in (22,23):\n",
    "        return '10pm-12am'\n",
    "    \n",
    "df['deadline_time'] = df.apply(two_hour_deadline, axis=1) # Calculates bins from launch_time\n",
    "\n",
    "df.drop('deadline_hour', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean pledge per backer\n",
    "df['pledge_per_backer'] = round(df['usd_pledged']/df['backers_count'],2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing null values for blurb_length with 0\n",
    "df.blurb_length.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping projects which are not successes or failures\n",
    "df = df[df['state'].isin(['successful', 'failed'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Of the 192664 projects in the dataset, there are 23685 which are listed more than once.\n",
      "Of these, 23674 have every value in common between duplicates.\n"
     ]
    }
   ],
   "source": [
    "# Checking for duplicates of individual projects, and sorting by id\n",
    "duplicates = df[df.duplicated(subset='id')]\n",
    "print(f\"Of the {len(df)} projects in the dataset, there are {len(df[df.duplicated(subset='id')])} which are listed more than once.\")\n",
    "print(f\"Of these, {len(df[df.duplicated()])} have every value in common between duplicates.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programmes files (x86)\\Anaconda\\envs\\tensorflow_env\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Dropping duplicates which have every value in common\n",
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicated = df[df.duplicated(subset='id', keep=False)].sort_values(by='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programmes files (x86)\\Anaconda\\envs\\tensorflow_env\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df.drop_duplicates(subset='id', keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>backers_count</th>\n",
       "      <th>category</th>\n",
       "      <th>country</th>\n",
       "      <th>created_at</th>\n",
       "      <th>deadline</th>\n",
       "      <th>is_starrable</th>\n",
       "      <th>launched_at</th>\n",
       "      <th>staff_pick</th>\n",
       "      <th>state</th>\n",
       "      <th>usd_pledged</th>\n",
       "      <th>blurb_length</th>\n",
       "      <th>sub_category</th>\n",
       "      <th>usd_goal</th>\n",
       "      <th>name_length</th>\n",
       "      <th>creation_to_launch_days</th>\n",
       "      <th>campaign_days</th>\n",
       "      <th>launch_day</th>\n",
       "      <th>deadline_day</th>\n",
       "      <th>launch_month</th>\n",
       "      <th>deadline_month</th>\n",
       "      <th>launch_time</th>\n",
       "      <th>deadline_time</th>\n",
       "      <th>pledge_per_backer</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>287514992</th>\n",
       "      <td>21</td>\n",
       "      <td>music</td>\n",
       "      <td>US</td>\n",
       "      <td>2013-12-21 21:01:30</td>\n",
       "      <td>2014-02-08 22:37:26</td>\n",
       "      <td>False</td>\n",
       "      <td>2013-12-25 22:37:26</td>\n",
       "      <td>False</td>\n",
       "      <td>successful</td>\n",
       "      <td>802.00</td>\n",
       "      <td>26.0</td>\n",
       "      <td>rock</td>\n",
       "      <td>200.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>45</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>December</td>\n",
       "      <td>February</td>\n",
       "      <td>10pm-12am</td>\n",
       "      <td>10pm-12am</td>\n",
       "      <td>38.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385129759</th>\n",
       "      <td>97</td>\n",
       "      <td>art</td>\n",
       "      <td>US</td>\n",
       "      <td>2019-02-08 21:02:48</td>\n",
       "      <td>2019-03-05 16:00:11</td>\n",
       "      <td>False</td>\n",
       "      <td>2019-02-13 16:00:11</td>\n",
       "      <td>False</td>\n",
       "      <td>successful</td>\n",
       "      <td>2259.00</td>\n",
       "      <td>9.0</td>\n",
       "      <td>mixed media</td>\n",
       "      <td>400.0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>February</td>\n",
       "      <td>March</td>\n",
       "      <td>4pm-6pm</td>\n",
       "      <td>4pm-6pm</td>\n",
       "      <td>23.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681033598</th>\n",
       "      <td>88</td>\n",
       "      <td>photography</td>\n",
       "      <td>US</td>\n",
       "      <td>2016-10-23 17:06:24</td>\n",
       "      <td>2016-12-01 15:58:50</td>\n",
       "      <td>False</td>\n",
       "      <td>2016-11-01 14:58:50</td>\n",
       "      <td>True</td>\n",
       "      <td>successful</td>\n",
       "      <td>29638.00</td>\n",
       "      <td>25.0</td>\n",
       "      <td>photobooks</td>\n",
       "      <td>27224.0</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>30</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>November</td>\n",
       "      <td>December</td>\n",
       "      <td>2pm-4pm</td>\n",
       "      <td>2pm-4pm</td>\n",
       "      <td>336.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1031782682</th>\n",
       "      <td>193</td>\n",
       "      <td>fashion</td>\n",
       "      <td>IT</td>\n",
       "      <td>2018-10-24 08:32:00</td>\n",
       "      <td>2018-12-08 22:59:00</td>\n",
       "      <td>False</td>\n",
       "      <td>2018-10-27 23:56:22</td>\n",
       "      <td>False</td>\n",
       "      <td>successful</td>\n",
       "      <td>49075.15</td>\n",
       "      <td>13.0</td>\n",
       "      <td>footwear</td>\n",
       "      <td>45461.0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>42</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>October</td>\n",
       "      <td>December</td>\n",
       "      <td>10pm-12am</td>\n",
       "      <td>10pm-12am</td>\n",
       "      <td>254.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>904085819</th>\n",
       "      <td>20</td>\n",
       "      <td>technology</td>\n",
       "      <td>US</td>\n",
       "      <td>2015-03-07 05:35:17</td>\n",
       "      <td>2015-04-08 16:36:57</td>\n",
       "      <td>False</td>\n",
       "      <td>2015-03-09 16:36:57</td>\n",
       "      <td>False</td>\n",
       "      <td>failed</td>\n",
       "      <td>549.00</td>\n",
       "      <td>22.0</td>\n",
       "      <td>software</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "      <td>Monday</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>March</td>\n",
       "      <td>April</td>\n",
       "      <td>4pm-6pm</td>\n",
       "      <td>4pm-6pm</td>\n",
       "      <td>27.45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            backers_count     category country          created_at  \\\n",
       "id                                                                   \n",
       "287514992              21        music      US 2013-12-21 21:01:30   \n",
       "385129759              97          art      US 2019-02-08 21:02:48   \n",
       "681033598              88  photography      US 2016-10-23 17:06:24   \n",
       "1031782682            193      fashion      IT 2018-10-24 08:32:00   \n",
       "904085819              20   technology      US 2015-03-07 05:35:17   \n",
       "\n",
       "                      deadline  is_starrable         launched_at  staff_pick  \\\n",
       "id                                                                             \n",
       "287514992  2014-02-08 22:37:26         False 2013-12-25 22:37:26       False   \n",
       "385129759  2019-03-05 16:00:11         False 2019-02-13 16:00:11       False   \n",
       "681033598  2016-12-01 15:58:50         False 2016-11-01 14:58:50        True   \n",
       "1031782682 2018-12-08 22:59:00         False 2018-10-27 23:56:22       False   \n",
       "904085819  2015-04-08 16:36:57         False 2015-03-09 16:36:57       False   \n",
       "\n",
       "                 state  usd_pledged  blurb_length sub_category  usd_goal  \\\n",
       "id                                                                         \n",
       "287514992   successful       802.00          26.0         rock     200.0   \n",
       "385129759   successful      2259.00           9.0  mixed media     400.0   \n",
       "681033598   successful     29638.00          25.0   photobooks   27224.0   \n",
       "1031782682  successful     49075.15          13.0     footwear   45461.0   \n",
       "904085819       failed       549.00          22.0     software    1000.0   \n",
       "\n",
       "            name_length  creation_to_launch_days  campaign_days launch_day  \\\n",
       "id                                                                           \n",
       "287514992             4                        4             45  Wednesday   \n",
       "385129759             5                        5             20  Wednesday   \n",
       "681033598             9                        9             30    Tuesday   \n",
       "1031782682            5                        4             42   Saturday   \n",
       "904085819             4                        2             30     Monday   \n",
       "\n",
       "           deadline_day launch_month deadline_month launch_time deadline_time  \\\n",
       "id                                                                              \n",
       "287514992      Saturday     December       February   10pm-12am     10pm-12am   \n",
       "385129759       Tuesday     February          March     4pm-6pm       4pm-6pm   \n",
       "681033598      Thursday     November       December     2pm-4pm       2pm-4pm   \n",
       "1031782682     Saturday      October       December   10pm-12am     10pm-12am   \n",
       "904085819     Wednesday        March          April     4pm-6pm       4pm-6pm   \n",
       "\n",
       "            pledge_per_backer  \n",
       "id                             \n",
       "287514992               38.19  \n",
       "385129759               23.29  \n",
       "681033598              336.80  \n",
       "1031782682             254.28  \n",
       "904085819               27.45  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting the id column as the index\n",
    "df.set_index('id', inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>country</th>\n",
       "      <th>staff_pick</th>\n",
       "      <th>state</th>\n",
       "      <th>blurb_length</th>\n",
       "      <th>usd_goal</th>\n",
       "      <th>name_length</th>\n",
       "      <th>creation_to_launch_days</th>\n",
       "      <th>campaign_days</th>\n",
       "      <th>launch_day</th>\n",
       "      <th>deadline_day</th>\n",
       "      <th>launch_month</th>\n",
       "      <th>deadline_month</th>\n",
       "      <th>launch_time</th>\n",
       "      <th>deadline_time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>287514992</th>\n",
       "      <td>music</td>\n",
       "      <td>US</td>\n",
       "      <td>False</td>\n",
       "      <td>successful</td>\n",
       "      <td>26.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>45</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>December</td>\n",
       "      <td>February</td>\n",
       "      <td>10pm-12am</td>\n",
       "      <td>10pm-12am</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385129759</th>\n",
       "      <td>art</td>\n",
       "      <td>US</td>\n",
       "      <td>False</td>\n",
       "      <td>successful</td>\n",
       "      <td>9.0</td>\n",
       "      <td>400.0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>February</td>\n",
       "      <td>March</td>\n",
       "      <td>4pm-6pm</td>\n",
       "      <td>4pm-6pm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681033598</th>\n",
       "      <td>photography</td>\n",
       "      <td>US</td>\n",
       "      <td>True</td>\n",
       "      <td>successful</td>\n",
       "      <td>25.0</td>\n",
       "      <td>27224.0</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>30</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>November</td>\n",
       "      <td>December</td>\n",
       "      <td>2pm-4pm</td>\n",
       "      <td>2pm-4pm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1031782682</th>\n",
       "      <td>fashion</td>\n",
       "      <td>IT</td>\n",
       "      <td>False</td>\n",
       "      <td>successful</td>\n",
       "      <td>13.0</td>\n",
       "      <td>45461.0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>42</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>October</td>\n",
       "      <td>December</td>\n",
       "      <td>10pm-12am</td>\n",
       "      <td>10pm-12am</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>904085819</th>\n",
       "      <td>technology</td>\n",
       "      <td>US</td>\n",
       "      <td>False</td>\n",
       "      <td>failed</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "      <td>Monday</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>March</td>\n",
       "      <td>April</td>\n",
       "      <td>4pm-6pm</td>\n",
       "      <td>4pm-6pm</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               category country  staff_pick       state  blurb_length  \\\n",
       "id                                                                      \n",
       "287514992         music      US       False  successful          26.0   \n",
       "385129759           art      US       False  successful           9.0   \n",
       "681033598   photography      US        True  successful          25.0   \n",
       "1031782682      fashion      IT       False  successful          13.0   \n",
       "904085819    technology      US       False      failed          22.0   \n",
       "\n",
       "            usd_goal  name_length  creation_to_launch_days  campaign_days  \\\n",
       "id                                                                          \n",
       "287514992      200.0            4                        4             45   \n",
       "385129759      400.0            5                        5             20   \n",
       "681033598    27224.0            9                        9             30   \n",
       "1031782682   45461.0            5                        4             42   \n",
       "904085819     1000.0            4                        2             30   \n",
       "\n",
       "           launch_day deadline_day launch_month deadline_month launch_time  \\\n",
       "id                                                                           \n",
       "287514992   Wednesday     Saturday     December       February   10pm-12am   \n",
       "385129759   Wednesday      Tuesday     February          March     4pm-6pm   \n",
       "681033598     Tuesday     Thursday     November       December     2pm-4pm   \n",
       "1031782682   Saturday     Saturday      October       December   10pm-12am   \n",
       "904085819      Monday    Wednesday        March          April     4pm-6pm   \n",
       "\n",
       "           deadline_time  \n",
       "id                        \n",
       "287514992      10pm-12am  \n",
       "385129759        4pm-6pm  \n",
       "681033598        2pm-4pm  \n",
       "1031782682     10pm-12am  \n",
       "904085819        4pm-6pm  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dropping columns and creating new dataframe\n",
    "df_transformed = df.drop(['backers_count', 'created_at', 'deadline', 'is_starrable', 'launched_at', 'usd_pledged', 'sub_category', 'pledge_per_backer'], axis=1)\n",
    "df_transformed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed['state'] = df_transformed['state'].replace({'failed': 0, 'successful': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting boolean features to string to include them in one-hot encoding\n",
    "df_transformed['staff_pick'] = df_transformed['staff_pick'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dummy variables\n",
    "df_transformed = pd.get_dummies(df_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us <font color = purple>**log-transform**</font> the data on the appropriate features. It allows better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assessing skewed distributions\n",
    "cols_to_log = ['creation_to_launch_days', 'name_length', 'usd_goal']\n",
    "# Replacing 0s with 0.01 and log-transforming\n",
    "for col in cols_to_log:\n",
    "    df_transformed[col] = df_transformed[col].astype('float64').replace(0.0, 0.01)\n",
    "    df_transformed[col] = np.log(df_transformed[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_transformed.drop('state', axis=1)\n",
    "y = df_transformed.state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally save the datasets (both obvservations and labels in seperate datasets) in the right sub folder so as not to redo the whole data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.to_csv('datasets/observations.csv', header=X.columns )\n",
    "y = y.values\n",
    "y = pd.DataFrame(y)\n",
    "y.to_csv('datasets/labels.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afterwards, we should separate the dataset in three similar subsets for further trainings, validation and tests. More specifically, one part will be used to train the level 0 models, the second part will be used to train the level 1 model and the last subset will be used to test the whole stack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = X.values.shape[0]\n",
    "n_subset = n_samples//3\n",
    "\n",
    "X1 = X.iloc[:n_subset,:]\n",
    "X2 = X.iloc[n_subset:2*n_subset,:]\n",
    "X3 = X.iloc[2*n_subset:,:]\n",
    "\n",
    "y1 = y.iloc[:n_subset,:]\n",
    "y2 = y.iloc[n_subset:2*n_subset,:]\n",
    "y3 = y.iloc[2*n_subset:,:];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1.to_csv('datasets/observations1.csv', header=X1.columns )\n",
    "X2.to_csv('datasets/observations2.csv', header=X2.columns )\n",
    "X3.to_csv('datasets/observations3.csv', header=X3.columns )\n",
    "y1.to_csv('datasets/labels1.csv')\n",
    "y2.to_csv('datasets/labels2.csv')\n",
    "y3.to_csv('datasets/labels3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training simple models without dimensionality reduction\n",
    "Now, we should train each of the model specified in the model notebook, on each of the subsets. However, note that the data was <font color=purple>**log-transformed**</font> to accomplish better results.\n",
    "#### Vanilla logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programmes files (x86)\\Anaconda\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "D:\\Programmes files (x86)\\Anaconda\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "D:\\Programmes files (x86)\\Anaconda\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "D:\\Programmes files (x86)\\Anaconda\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "D:\\Programmes files (x86)\\Anaconda\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "D:\\Programmes files (x86)\\Anaconda\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting logistic regression models with default parameters\n",
    "logreg1 = LogisticRegression()\n",
    "logreg1.fit(X1,y1)\n",
    "logreg2 = LogisticRegression()\n",
    "logreg2.fit(X2,y2)\n",
    "logreg3 = LogisticRegression()\n",
    "logreg3.fit(X3,y3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we should save the trained models in the proper sub folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_list = [logreg1,logreg2,logreg3]\n",
    "\n",
    "for i in range(3):\n",
    "    with open('stacking_test/logreg'+str(i+1)+'.txt','wb') as fichier:\n",
    "        pickler = pk.Pickler(fichier)\n",
    "        pickler.dump(logistic_list[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programmes files (x86)\\Anaconda\\envs\\tensorflow_env\\lib\\site-packages\\ipykernel_launcher.py:2: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  \n",
      "D:\\Programmes files (x86)\\Anaconda\\envs\\tensorflow_env\\lib\\site-packages\\ipykernel_launcher.py:4: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  after removing the cwd from sys.path.\n",
      "D:\\Programmes files (x86)\\Anaconda\\envs\\tensorflow_env\\lib\\site-packages\\ipykernel_launcher.py:6: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=35, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=0.001,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=400,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf1 = RandomForestClassifier(max_depth=35, min_samples_split=0.001, n_estimators=400)\n",
    "rf1.fit(X1, y1)\n",
    "rf2 = RandomForestClassifier(max_depth=35, min_samples_split=0.001, n_estimators=400)\n",
    "rf2.fit(X2, y2)\n",
    "rf3 = RandomForestClassifier(max_depth=35, min_samples_split=0.001, n_estimators=400)\n",
    "rf3.fit(X3, y3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_list = [rf1,rf2,rf3]\n",
    "for i in range(3):\n",
    "    with open('stacking_test/random_forest_'+str(i+1)+'.txt','wb') as fichier:\n",
    "        pickler = pk.Pickler(fichier)\n",
    "        pickler.dump(rf_list[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programmes files (x86)\\Anaconda\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:235: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "D:\\Programmes files (x86)\\Anaconda\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:268: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
       "              learning_rate=0.1, max_delta_step=0, max_depth=35,\n",
       "              min_child_weight=100, missing=None, n_estimators=100, n_jobs=1,\n",
       "              nthread=None, objective='binary:logistic', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "              silent=None, subsample=0.7, verbosity=1)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb1 = xgb.XGBClassifier(learning_rate=0.1, max_depth=35, min_child_weight=100, n_estimators=100, subsample=0.7)\n",
    "xgb1.fit(X1, y1)\n",
    "xgb2 = xgb.XGBClassifier(learning_rate=0.1, max_depth=35, min_child_weight=100, n_estimators=100, subsample=0.7)\n",
    "xgb2.fit(X2, y2)\n",
    "xgb3 = xgb.XGBClassifier(learning_rate=0.1, max_depth=35, min_child_weight=100, n_estimators=100, subsample=0.7)\n",
    "xgb3.fit(X3, y3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_list = [xgb1,xgb2,xgb3]\n",
    "for i in range(3):\n",
    "    with open('stacking_test/xgb_'+str(i+1)+'.txt','wb') as fichier:\n",
    "        pickler = pk.Pickler(fichier)\n",
    "        pickler.dump(xgb_list[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking without dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression as LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stacked_dataset(models, inputX):\n",
    "    \"\"\"\n",
    "    Input : list of learners, np.array\n",
    "    Output: np.array\n",
    "    The function takes a list of pretrained models and the training observations to return the concatenated predictions\n",
    "    of each and every model in a flattened array. The output will be the input of the level 1 model to train with\n",
    "    trainStack\n",
    "    \"\"\"\n",
    "    stackX = None\n",
    "    for model in models:\n",
    "        # make prediction\n",
    "        yhat = model.predict(inputX)\n",
    "        # stack predictions into [rows, members, probabilities]\n",
    "        if stackX is None:\n",
    "            stackX = yhat\n",
    "        else:\n",
    "            stackX = np.dstack((stackX, yhat))\n",
    "    # flatten predictions to [rows, members x probabilities]\n",
    "    stackX = stackX.reshape((stackX.shape[1], stackX.shape[2]))\n",
    "    print(\"Il y a {0} modèles, le format des observations est : {1} et celui des observations empilées est : {2}\".format(len(models), inputX.shape, stackX.shape))\n",
    "    print(\"Les cinq premières lignes ressemblent à ceci : {}\".format(stackX[:5,:]))\n",
    "    return stackX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainStack(first_models, final_model, X_train, y_train):\n",
    "    \"\"\"\n",
    "    Input : list of learners, learner, np.array, np.array\n",
    "    Output : learner\n",
    "    The function takes the level 0 trained learners, the level 1 learner to train, the training observations and the \n",
    "    training labels. It returns the level 1 trained model.\n",
    "    \"\"\"\n",
    "    \n",
    "    X_stacked = stacked_dataset(first_models, X_train)\n",
    "    final_model.fit(X_stacked, y_train)\n",
    "    \n",
    "    return final_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictStack(first_models, final_model, X_test):\n",
    "    \"\"\"\n",
    "    Input : list of learners, learner, array-like\n",
    "    Output : array-like\n",
    "    The function takes the first-level trained models, the top-level trained model and the test set and returns \n",
    "    the predictions of the stack on the test set.\n",
    "    \"\"\"\n",
    "    X_stacked = stacked_dataset(first_models, X_test)\n",
    "    y_predicted = final_model.predict(X_stacked)\n",
    "    return y_predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic regression\n",
    "We will use the logistic regression as the top-level trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the pretrained models\n",
    "logreg1, rf1, xgb1 = None, None, None\n",
    "\n",
    "with open('stacking_test/logreg1.txt','rb') as fichier:\n",
    "    pickler = pk.Unpickler(fichier)\n",
    "    logreg1 = pickler.load()\n",
    "\n",
    "with open('stacking_test/random_forest_1.txt','rb') as fichier:\n",
    "    pickler = pk.Unpickler(fichier)\n",
    "    rf1 = pickler.load()\n",
    "    \n",
    "with open('stacking_test/xgb_1.txt','rb') as fichier:\n",
    "    pickler = pk.Unpickler(fichier)\n",
    "    xgb1 = pickler.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the datasets. Since the models were trained on the first part of the dataset, we must choose different datasets this\n",
    "# time, for instance the second subsets\n",
    "X_train = pd.read_csv('datasets/observations2.csv')\n",
    "y_train = pd.read_csv('datasets/labels2.csv')\n",
    "X_train.drop('id',axis=1,inplace=True)\n",
    "y_train.drop('Unnamed: 0',axis=1,inplace=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_model1 = LogisticRegression();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Il y a 3 modèles, le format des observations est : (56326, 106) et celui des observations empilées est : (56326, 3)\n",
      "Les cinq premières lignes ressemblent à ceci : [[0 0 0]\n",
      " [1 1 0]\n",
      " [1 1 1]\n",
      " [0 0 0]\n",
      " [1 1 1]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programmes files (x86)\\Anaconda\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "stack1 = trainStack([logreg1, rf1, xgb1], top_model1, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it's time to import the test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.read_csv('datasets/observations3.csv')\n",
    "y_test = pd.read_csv('datasets/labels3.csv')\n",
    "X_test.drop('id',axis=1,inplace=True)\n",
    "y_test.drop('Unnamed: 0',axis=1,inplace=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Il y a 3 modèles, le format des observations est : (56327, 106) et celui des observations empilées est : (56327, 3)\n",
      "Les cinq premières lignes ressemblent à ceci : [[1 1 1]\n",
      " [1 1 1]\n",
      " [1 1 1]\n",
      " [1 1 1]\n",
      " [1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "y_predicted = predictStack([logreg1, rf1, xgb1], stack1, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.747522</td>\n",
       "      <td>0.736698</td>\n",
       "      <td>0.731683</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Precision    Recall  F1_score\n",
       "0   0.747522  0.736698  0.731683"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Displaying the results\n",
    "stack1_test_precision, stack1_test_recall, stack1_test_f1score, stack1_test_support = precision_recall_fscore_support(y_test, y_predicted, average='weighted')\n",
    "stack1_results = {'Precision':[stack1_test_precision], 'Recall':[stack1_test_recall], 'F1_score': [stack1_test_f1score]}\n",
    "stack1_results = pd.DataFrame(stack1_results)\n",
    "stack1_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should renew the experiment with models trained on different datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
